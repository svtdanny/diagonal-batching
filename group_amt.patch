diff --git a/modeling_amt/language_modeling.py b/modeling_amt/language_modeling.py
index 9805263..6366aeb 100644
--- a/modeling_amt/language_modeling.py
+++ b/modeling_amt/language_modeling.py
@@ -60,6 +60,11 @@ class AssociativeLayerWrapper(torch.nn.Module):
         self.generate_mode = False
         self.first_seg = True
         self.correction = correction
+        
+        self._skip_associating = False
+        self._grouped_execution = False
+        self._first_seg_mask = "None" # use str placeholder to avoid None indexing
+        self._need_to_update_mem = "None"
 
     def associate(self, hidden_states):
 
@@ -78,7 +83,7 @@ class AssociativeLayerWrapper(torch.nn.Module):
         return hidden_states
     
     def forward(self, hidden_states, **kwargs):
-        if not self.first_seg:
+        if not self.first_seg and not self._skip_associating:
             hidden_states = self.associate(
                 # self.ln(
                     hidden_states
@@ -98,7 +103,20 @@ class AssociativeLayerWrapper(torch.nn.Module):
 
         mk = self.phi(self.W_mk(mem_tokens))
         new_mv = self.W_mv(mem_tokens) # (bsz, num_mem_tokens, d_model)
-        if not self.first_seg:
+        if not self._grouped_execution:
+            if not self.first_seg:
+                num = torch.einsum('ijk,ikt->ijt', mk, self.W_mem)
+                denom = torch.einsum("ij,ikj->ik", self.z, mk)[..., None] + 1e-5
+                prev_mv = num / denom
+                if self.correction:
+                    new_info_coef = 1 - denom / (torch.linalg.norm(mk, dim=-1) ** 2 + 1e-5)[..., None]
+                    new_info_coef = torch.clip(new_info_coef, 0, 1).detach()
+                else:
+                    new_info_coef = 1
+            else: 
+                prev_mv = torch.zeros_like(new_mv, device=new_mv.device)
+                new_info_coef = 1
+        else:
             num = torch.einsum('ijk,ikt->ijt', mk, self.W_mem)
             denom = torch.einsum("ij,ikj->ik", self.z, mk)[..., None] + 1e-5
             prev_mv = num / denom
@@ -106,10 +124,10 @@ class AssociativeLayerWrapper(torch.nn.Module):
                 new_info_coef = 1 - denom / (torch.linalg.norm(mk, dim=-1) ** 2 + 1e-5)[..., None]
                 new_info_coef = torch.clip(new_info_coef, 0, 1).detach()
             else:
-                new_info_coef = 1
-        else: 
-            prev_mv = torch.zeros_like(new_mv, device=new_mv.device)
-            new_info_coef = 1
+                new_info_coef = torch.ones((self.W_mem.data.shape[0],), device=self.W_mem.data.device)
+            
+            prev_mv[self._first_seg_mask] = 0
+            new_info_coef[self._first_seg_mask] = 1
         
         # wandb.log({f"gamma_{self.info['layer']}": new_info_coef.mean(dim=1).item() if isinstance(new_info_coef, torch.Tensor) else 1}, step=self.seg_num)
         mv = new_mv - prev_mv
@@ -120,12 +138,22 @@ class AssociativeLayerWrapper(torch.nn.Module):
         # new_info_coef = 1 - denom
 
         mb = torch.sigmoid(self.W_mb(mem_tokens))[..., 0]
-
         associations =  torch.einsum('ijk,ijt,ij->ikt', mk, mv, mb) # (bsz, d_mem, d_model)
-        self.W_mem = self.W_mem + associations
 
-        self.z = self.z + (new_info_coef*mk).sum(dim=1)
-        # self.z = self.z + (new_info_coef*mb[..., None]*mk).sum(dim=1)
+        if not self._grouped_execution:
+            self.W_mem = self.W_mem + associations
+
+            self.z = self.z + (new_info_coef*mk).sum(dim=1)
+            # self.z = self.z + (new_info_coef*mb[..., None]*mk).sum(dim=1)
+            
+            # print("self.W_mem: ", self.W_mem.sum())
+            # print("self.z: ", self.z)
+        else:
+            # self.W_mem[self._need_to_update_mem] = self.W_mem[self._need_to_update_mem] + associations[self._need_to_update_mem]
+            # self.z[self._need_to_update_mem] = self.z[self._need_to_update_mem] + (new_info_coef*mk).sum(dim=1)[self._need_to_update_mem]
+            self.W_mem[self._need_to_update_mem] += associations[self._need_to_update_mem]
+            self.z[self._need_to_update_mem] += (new_info_coef*mk)[self._need_to_update_mem].sum(dim=1)
+
         self.seg_num += 1
 
 
@@ -332,11 +360,12 @@ class AssociativeRecurrentWrapper(torch.nn.Module):
         self.memory_cell.zero_mem()
         for seg_num, segment in enumerate(segmented):
             seg_len = segment['input_ids'].size(-1)
+            # print(f"seg_len: {seg_len}")
             cell_out = self.memory_cell(**segment,  
                                         output_hidden_states=True, 
                                         use_cache=sliding_window, 
                                         past_key_values=past_key_values,
-                                        prev_attn_mask=prev_attn_mask,
+                                        # prev_attn_mask=prev_attn_mask,
                                         zero_mem=False
             )
             if sliding_window:
