{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "sys.path.append(\"./associative-recurrent-memory-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/svtdanny/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from grouped_batching.llama1b_grouping import (\n",
    "    wrap_model_with_armt, get_grouped_states, \n",
    "    make_grouped_layer_from_single_layer, make_grouped_model_from_naive,\n",
    "    make_grouped_sliced_layer_from_single_layer\n",
    ")\n",
    "from grouped_batching.fast_executor import (\n",
    "    FastGroupedArmtExecutor, GroupedLayerContext, \n",
    "    associate_with_context, update_mem_with_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grouped_batching.universal_grouping import (\n",
    "    extract_params_from_module, get_universal_grouped_states,\n",
    "    get_module_by_path, make_universal_grouped_layer,\n",
    "    set_module_by_path, make_universal_grouped_model\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.bfloat16\n",
    "torch.set_default_dtype(dtype)\n",
    "torch.set_grad_enabled(False)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import traceback\n",
    "\n",
    "def add_trace_to_forward(module, msg):\n",
    "    original_forward = module.forward\n",
    "\n",
    "    def traced_forward(*args, **kwargs):\n",
    "        print(f\"\\n[TRACE] {msg} {module.__class__.__name__}.forward called from:\\n\" + ''.join(traceback.format_stack(limit=10)))\n",
    "        return original_forward(*args, **kwargs)\n",
    "\n",
    "    module.forward = traced_forward\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import copy\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "source_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\"\n",
    "                                             , attn_implementation=\"flash_attention_2\"\n",
    "                                             ,torch_dtype=dtype)\n",
    "\n",
    "# Replace all LayerNorm modules in the model with LayerNorm without weights and biases\n",
    "# for name, module in source_model.named_modules():\n",
    "#     if isinstance(module, nn.LayerNorm):\n",
    "#         # Create a new LayerNorm with elementwise_affine=False (no weights and biases)\n",
    "#         new_layernorm = nn.LayerNorm(\n",
    "#             normalized_shape=module.normalized_shape,\n",
    "#             eps=module.eps,\n",
    "#             elementwise_affine=False\n",
    "#         )\n",
    "        \n",
    "#         # Get the parent module and attribute name to replace the LayerNorm\n",
    "#         parent_name = '.'.join(name.split('.')[:-1])\n",
    "#         child_name = name.split('.')[-1]\n",
    "        \n",
    "#         if parent_name:\n",
    "#             parent = source_model\n",
    "#             for part in parent_name.split('.'):\n",
    "#                 parent = getattr(parent, part)\n",
    "#             setattr(parent, child_name, new_layernorm)\n",
    "#         else:\n",
    "#             setattr(source_model, child_name, new_layernorm)\n",
    "\n",
    "# for l in source_model.transformer.h:\n",
    "    # l.mlp = nn.Identity()\n",
    "    # l.attn.attention.out_proj = nn.Identity()\n",
    "    # l.attn.attention = nn.Identity()\n",
    "\n",
    "# source_model.transformer.h = source_model.transformer.h[:1]\n",
    "\n",
    "# source_model.transformer.wpe.weight.fill_(0)\n",
    "# source_model.transformer.wpe.weight.data = torch.tensor([3,3,3], dtype=dtype)\n",
    "# source_model.transformer.wte.weight.fill_(0)\n",
    "# source_model.transformer.wte.weight.data = torch.tensor([3,3,3], dtype=dtype)\n",
    "# add_trace_to_forward(source_model.transformer.wpe, 'wpe')\n",
    "# add_trace_to_forward(source_model.transformer.wte, 'wte')\n",
    "\n",
    "\n",
    "# source_model.ln_f = source_model.transformer.ln_f\n",
    "# source_model.transformer.ln_f = nn.Identity()\n",
    "# source_model.lm_head = nn.Identity()\n",
    "reference_model = copy.deepcopy(source_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoFlashAttention2(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "armt_config = dict(\n",
    "    segment_size=1024,\n",
    "    num_mem_tokens=128,\n",
    "    d_mem=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoFlashAttention2(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "armt_model = wrap_model_with_armt(source_model, **armt_config, layers_attr='transformer.h')\n",
    "armt_model.to(\"cuda\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "armt_reference_model = wrap_model_with_armt(reference_model, **armt_config, layers_attr='transformer.h')\n",
    "armt_reference_model.to(\"cuda\")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_params = get_universal_grouped_states(armt_model.memory_cell.model.transformer.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W_mq.weight',\n",
       " 'W_mk.weight',\n",
       " 'W_mv.weight',\n",
       " 'W_mb.weight',\n",
       " 'W_mb.bias',\n",
       " 'layer.ln_1.weight',\n",
       " 'layer.ln_1.bias',\n",
       " 'layer.attn.attention.k_proj.weight',\n",
       " 'layer.attn.attention.v_proj.weight',\n",
       " 'layer.attn.attention.q_proj.weight',\n",
       " 'layer.attn.attention.out_proj.weight',\n",
       " 'layer.attn.attention.out_proj.bias',\n",
       " 'layer.ln_2.weight',\n",
       " 'layer.ln_2.bias',\n",
       " 'layer.mlp.c_fc.weight',\n",
       " 'layer.mlp.c_fc.bias',\n",
       " 'layer.mlp.c_proj.weight',\n",
       " 'layer.mlp.c_proj.bias',\n",
       " 'W_mem',\n",
       " 'z']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(grouped_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_context = GroupedLayerContext()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBSTITUTE efficient module_path='W_mq': len(weight_values)=12 None \n",
      "SUBSTITUTE efficient module_path='W_mk': len(weight_values)=12 None \n",
      "SUBSTITUTE efficient module_path='W_mv': len(weight_values)=12 None \n",
      "SUBSTITUTE naive module_path='W_mb': len(weight_values)=12 12 \n",
      "SUBSTITUTE efficient module_path='layer.attn.attention.k_proj': len(weight_values)=12 None \n",
      "SUBSTITUTE efficient module_path='layer.attn.attention.v_proj': len(weight_values)=12 None \n",
      "SUBSTITUTE efficient module_path='layer.attn.attention.q_proj': len(weight_values)=12 None \n",
      "SUBSTITUTE efficient module_path='layer.attn.attention.out_proj': len(weight_values)=12 12 \n",
      "SUBSTITUTE efficient module_path='layer.mlp.c_fc': len(weight_values)=12 12 \n",
      "SUBSTITUTE efficient module_path='layer.mlp.c_proj': len(weight_values)=12 12 \n",
      "SUBSTITUTE norm_path='layer.ln_1': torch.Size([12, 768]) torch.Size([12, 768]) \n",
      "SUBSTITUTE norm_path='layer.ln_2': torch.Size([12, 768]) torch.Size([12, 768]) \n"
     ]
    }
   ],
   "source": [
    "grouped_layer = make_universal_grouped_layer(\n",
    "    grouped_context, \n",
    "    copy.deepcopy(armt_model.memory_cell.model.transformer.h[0]),\n",
    "    grouped_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x AssociativeLayerWrapper(\n",
       "        (W_mq): Linear(in_features=768, out_features=64, bias=False)\n",
       "        (W_mk): Linear(in_features=768, out_features=64, bias=False)\n",
       "        (W_mv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_mb): Linear(in_features=768, out_features=1, bias=True)\n",
       "        (layer): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoFlashAttention2(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 384, 768]), torch.Size([12, 384]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_layer.W_mem.data.shape, grouped_layer.z.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_layer.layer.ln_1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_layer.layer.ln_1.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "def zero_grouped_memory(self):\n",
    "    \"\"\"Zero out the memory of a grouped ARMT model.\"\"\"\n",
    "    self.W_mem.detach_()\n",
    "    self.W_mem.fill_(0)\n",
    "    self.z.detach_()\n",
    "    self.z.fill_(0)\n",
    "\n",
    "class UniversalGroupedExecutor(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Universal grouped executor for ARMT models.\n",
    "    \n",
    "    This class provides a flexible executor that can work with any model structure\n",
    "    by using configurable paths to model components.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        grouped_layer, \n",
    "        context, \n",
    "        n_layers, \n",
    "        model_path=\"memory_cell.model.model\",\n",
    "        out_norm_attr=\"out_norm\",\n",
    "        lm_head_path=\"memory_cell.model.lm_head\",\n",
    "        memory_path=\"memory_cell\",\n",
    "        segment_fn=None,\n",
    "        process_input_fn=None,\n",
    "        vanilla_model=None,\n",
    "        preprocess_segment_fn = None,\n",
    "        postprocess_segment_fn = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the universal grouped executor.\n",
    "        \n",
    "        Args:\n",
    "            model: The model to execute\n",
    "            grouped_layer: The grouped layer\n",
    "            context: The context for grouped execution\n",
    "            n_layers: Number of layers in the model\n",
    "            model_path: Path to the main model module\n",
    "            out_norm_attr: Attribute name where the output norm is stored\n",
    "            lm_head_path: Path to the language model head\n",
    "            memory_path: Path to the memory cell module\n",
    "            segment_fn: Function to segment inputs (if None, uses model.segment)\n",
    "            process_input_fn: Function to process inputs (if None, uses memory_cell.process_input)\n",
    "            vanilla_model: Original model for generation (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.grouped_layer = grouped_layer\n",
    "        self.context = context\n",
    "        self.n_layers = n_layers\n",
    "        self.vanilla_model = vanilla_model\n",
    "        \n",
    "        self.preprocess_segment_fn = preprocess_segment_fn\n",
    "        self.postprocess_segment_fn = postprocess_segment_fn\n",
    "        \n",
    "        # Store paths\n",
    "        self.model_path = model_path\n",
    "        self.out_norm_attr = out_norm_attr\n",
    "        self.lm_head_path = lm_head_path\n",
    "        self.memory_path = memory_path\n",
    "        \n",
    "        # Get components by path\n",
    "        self.base_model = get_module_by_path(model, model_path)\n",
    "        if self.base_model is None:\n",
    "            raise ValueError(f\"Could not find base model at path: {model_path}\")\n",
    "\n",
    "        self.out_norm = get_module_by_path(model, out_norm_attr)\n",
    "        if self.out_norm is None:\n",
    "            print(f\"Warning: Could not find out norm at path: {out_norm_attr}\")\n",
    "            \n",
    "        self.lm_head = get_module_by_path(model, lm_head_path)\n",
    "        if self.lm_head is None:\n",
    "            print(f\"Warning: Could not find LM head at path: {lm_head_path}\")\n",
    "            \n",
    "        self.memory_cell = get_module_by_path(model, memory_path)\n",
    "        if self.memory_cell is None:\n",
    "            raise ValueError(f\"Could not find memory cell at path: {memory_path}\")\n",
    "            \n",
    "        # Segmentation and input processing functions\n",
    "        self.segment_fn = segment_fn if segment_fn is not None else model.segment\n",
    "        \n",
    "        if process_input_fn is not None:\n",
    "            self.process_input_fn = process_input_fn\n",
    "        elif hasattr(self.memory_cell, 'process_input'):\n",
    "            self.process_input_fn = self.memory_cell.process_input\n",
    "        else:\n",
    "            raise ValueError(\"No process_input function provided or found\")\n",
    "            \n",
    "        # Set generation mode\n",
    "        self.grouped_layer.generate_mode = True\n",
    "    \n",
    "    def forward(self, input_ids, skip_concat=False):\n",
    "        \"\"\"\n",
    "        Forward pass for the grouped model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Input token IDs\n",
    "            skip_concat: Whether to skip concatenating outputs\n",
    "            \n",
    "        Returns:\n",
    "            Model outputs\n",
    "        \"\"\"\n",
    "        self.context.is_full = False\n",
    "        self.context.start_idx = 0\n",
    "        self.context.end_idx = 0\n",
    "        \n",
    "        # Zero out memory\n",
    "        zero_grouped_memory(self.grouped_layer)\n",
    "        \n",
    "        # Segment inputs\n",
    "        input_segments = [iseg for iseg in self.segment_fn(input_ids=input_ids)]\n",
    "        segments = [self.process_input_fn(**iseg)['inputs_embeds'][0] for iseg in input_segments]\n",
    "        \n",
    "        segment_outputs = []\n",
    "        grouped_input = []\n",
    "        \n",
    "        for i in range(self.n_layers + len(segments) - 1):\n",
    "            if i < len(segments):\n",
    "                # Add new segment until have one \n",
    "                new_segment = segments[i]\n",
    "                if self.preprocess_segment_fn is not None:\n",
    "                    new_segment = self.preprocess_segment_fn(self.model, new_segment)\n",
    "                grouped_input.insert(0, new_segment)\n",
    "                \n",
    "            grouped_input_tensor = torch.stack(grouped_input).contiguous()\n",
    "            if i < self.n_layers:\n",
    "                # Compute before end_idx+=1 to skip first segment association\n",
    "                if i > 0 and grouped_input_tensor.shape[0] > 1:\n",
    "                    grouped_input_tensor[:-1, ...] += associate_with_context(self.grouped_layer, self.context, grouped_input_tensor[:-1, ...])\n",
    "                \n",
    "                # Allow more weights to be computed\n",
    "                self.context.end_idx += 1\n",
    "                if self.context.end_idx == self.n_layers and self.context.start_idx == 0:\n",
    "                    self.context.is_full = True\n",
    "            else:\n",
    "                grouped_input_tensor += associate_with_context(self.grouped_layer, self.context, grouped_input_tensor)\n",
    "            \n",
    "            # Process through the grouped layer\n",
    "            grouped_output = self.grouped_layer.forward(grouped_input_tensor)\n",
    "            grouped_output = grouped_output[0]\n",
    "            \n",
    "            # Update memory with context\n",
    "            update_mem_with_context(self.grouped_layer, self.context, grouped_output[:, -self.grouped_layer.num_mem_tokens:])\n",
    "            \n",
    "            grouped_input = list(grouped_output.unbind(0))\n",
    "            if i >= self.n_layers - 1:\n",
    "                segment_out_logits = grouped_input.pop(-1)\n",
    "                \n",
    "                \n",
    "                processed_segment = segment_out_logits[:-self.grouped_layer.num_mem_tokens]\n",
    "                if self.postprocess_segment_fn is not None:\n",
    "                    processed_segment = self.postprocess_segment_fn(self.model, processed_segment)\n",
    "                \n",
    "                segment_outputs.append(processed_segment)\n",
    "                \n",
    "            if i >= len(segments) - 1:\n",
    "                # Reduce number of weights to be computed\n",
    "                self.context.start_idx += 1\n",
    "                self.context.is_full = False\n",
    "              \n",
    "        if skip_concat:\n",
    "            return segment_outputs\n",
    "        \n",
    "        # Concatenate outputs\n",
    "        output = torch.cat(segment_outputs, dim=0)\n",
    "        \n",
    "        # Return as a CausalLMOutput\n",
    "        return transformers.modeling_outputs.CausalLMOutputWithPast(\n",
    "            logits=output,\n",
    "        )\n",
    "    \n",
    "    def generate(self, input_ids, attention_mask, seg_size, **generate_kwargs):\n",
    "        \"\"\"\n",
    "        Generate text using the model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Input token IDs\n",
    "            attention_mask: Attention mask\n",
    "            seg_size: Segment size\n",
    "            **generate_kwargs: Additional keyword arguments for generation\n",
    "            \n",
    "        Returns:\n",
    "            Generated output and copy time\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        # Process the vanilla model if available\n",
    "        if self.vanilla_model is not None:\n",
    "            vanilla_memory_cell = get_module_by_path(self.vanilla_model, self.memory_path)\n",
    "            if vanilla_memory_cell is not None:\n",
    "                vanilla_memory_cell.zero_mem()\n",
    "        elif hasattr(self.memory_cell, 'zero_mem'):\n",
    "            self.memory_cell.zero_mem()\n",
    "            \n",
    "        # Handle large inputs\n",
    "        if input_ids.shape[-1] > seg_size - self.grouped_layer.num_mem_tokens:\n",
    "            # Cut last part of the segment\n",
    "            last_segm = input_ids.shape[-1] // (seg_size - self.grouped_layer.num_mem_tokens) * (seg_size - self.grouped_layer.num_mem_tokens)\n",
    "            prev_ids = input_ids[..., :last_segm]\n",
    "            last_ids = input_ids[..., last_segm:]\n",
    "            last_attn_mask = attention_mask[..., last_segm:]\n",
    "            \n",
    "            # Process previous segments\n",
    "            _ = self.forward(prev_ids)\n",
    "            \n",
    "            # Process last segment\n",
    "            segmented = self.segment_fn(input_ids=last_ids, attention_mask=last_attn_mask)\n",
    "            final_segment = segmented[-1]\n",
    "            \n",
    "            # Use vanilla model for generation if available\n",
    "            if self.vanilla_model is not None:\n",
    "                vanilla_memory_cell = get_module_by_path(self.vanilla_model, self.memory_path)\n",
    "                if vanilla_memory_cell is not None:\n",
    "                    # Patch memory\n",
    "                    time_start = time.time()\n",
    "                    vanilla_memory_cell.memory = self.memory_cell.memory\n",
    "                    \n",
    "                    # Copy weights\n",
    "                    for idx in range(len(vanilla_memory_cell.layers)):\n",
    "                        if hasattr(self.grouped_layer, 'W_mem'):\n",
    "                            vanilla_memory_cell.layers[idx].W_mem = self.grouped_layer.W_mem[idx].unsqueeze(0)\n",
    "                        if hasattr(self.grouped_layer, 'z'):\n",
    "                            vanilla_memory_cell.layers[idx].z = self.grouped_layer.z[idx].unsqueeze(0)\n",
    "                        vanilla_memory_cell.layers[idx].first_seg = False\n",
    "                    \n",
    "                    time_end = time.time()\n",
    "                    out = vanilla_memory_cell.generate(**final_segment, zero_mem=False, **generate_kwargs)\n",
    "                    vanilla_memory_cell.zero_mem()\n",
    "                    copy_time = time_end - time_start\n",
    "                    return out, copy_time\n",
    "            \n",
    "            # Use memory cell for generation\n",
    "            if hasattr(self.memory_cell, 'generate'):\n",
    "                out = self.memory_cell.generate(**final_segment, zero_mem=False, **generate_kwargs)\n",
    "                if hasattr(self.memory_cell, 'zero_mem'):\n",
    "                    self.memory_cell.zero_mem()\n",
    "                return out, 0\n",
    "        else:\n",
    "            # Process inputs directly\n",
    "            segmented = self.segment_fn(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            final_segment = segmented[-1]\n",
    "            \n",
    "            # Use vanilla model for generation\n",
    "            if self.vanilla_model is not None:\n",
    "                vanilla_memory_cell = get_module_by_path(self.vanilla_model, self.memory_path)\n",
    "                if vanilla_memory_cell is not None:\n",
    "                    out = vanilla_memory_cell.generate(**final_segment, zero_mem=False, **generate_kwargs)\n",
    "                    vanilla_memory_cell.zero_mem()\n",
    "                    return out, 0\n",
    "            \n",
    "            # Use memory cell for generation\n",
    "            if hasattr(self.memory_cell, 'generate'):\n",
    "                out = self.memory_cell.generate(**final_segment, zero_mem=False, **generate_kwargs)\n",
    "                if hasattr(self.memory_cell, 'zero_mem'):\n",
    "                    self.memory_cell.zero_mem()\n",
    "                return out, 0\n",
    "        \n",
    "        # Fallback\n",
    "        raise ValueError(\"Could not generate output - no suitable generation method found\")\n",
    "    \n",
    "    def to(self, device):\n",
    "        \"\"\"Move model to device.\"\"\"\n",
    "        self.model.to(device)\n",
    "        self.grouped_layer.to(device)\n",
    "        if self.vanilla_model is not None:\n",
    "            self.vanilla_model.to(device)\n",
    "        return self\n",
    "    \n",
    "    def eval(self):\n",
    "        \"\"\"Set model to evaluation mode.\"\"\"\n",
    "        self.model.eval()\n",
    "        self.grouped_layer.eval()\n",
    "        if self.vanilla_model is not None:\n",
    "            self.vanilla_model.eval()\n",
    "        return self\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Set model to training mode.\"\"\"\n",
    "        self.model.train()\n",
    "        self.grouped_layer.train()\n",
    "        if self.vanilla_model is not None:\n",
    "            self.vanilla_model.train()\n",
    "        return self "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x AssociativeLayerWrapper(\n",
       "        (W_mq): Linear(in_features=768, out_features=64, bias=False)\n",
       "        (W_mk): Linear(in_features=768, out_features=64, bias=False)\n",
       "        (W_mv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_mb): Linear(in_features=768, out_features=1, bias=True)\n",
       "        (layer): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoFlashAttention2(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "armt_grouped_model, source_model_layers = make_universal_grouped_model(\n",
    "    armt_model, \n",
    "    grouped_layer,\n",
    "    layers_path=\"memory_cell.model.transformer.h\"\n",
    ")\n",
    "\n",
    "def preprocess_segment(model, input_segment):\n",
    "    pos_ids = torch.arange(\n",
    "        0, \n",
    "        input_segment.shape[0], \n",
    "        device=input_segment.device\n",
    "    )\n",
    "    wpe_emb = model.memory_cell.model.transformer.wpe(pos_ids)\n",
    "    input_segment.add_(wpe_emb)\n",
    "    return input_segment\n",
    "\n",
    "def postprocess_segment(model, output_segment):\n",
    "    out = model.memory_cell.model.transformer.ln_f(output_segment)\n",
    "    out_tokens = model.memory_cell.model.lm_head(out)\n",
    "    return out_tokens\n",
    "\n",
    "executor = UniversalGroupedExecutor(\n",
    "    model=armt_grouped_model,\n",
    "    grouped_layer=grouped_layer,\n",
    "    context=grouped_context,\n",
    "    n_layers=source_model.config.num_hidden_layers,\n",
    "    model_path=\"memory_cell.model.transformer\",\n",
    "    out_norm_attr=\"memory_cell.model.transformer.ln_f\",\n",
    "    lm_head_path=\"memory_cell.model.lm_head\",\n",
    "    memory_path=\"memory_cell\",\n",
    "    preprocess_segment_fn = preprocess_segment,\n",
    "    postprocess_segment_fn = postprocess_segment,\n",
    ")\n",
    "\n",
    "\n",
    "# executor = FastGroupedArmtExecutor(\n",
    "#     armt_grouped_model, \n",
    "#     grouped_layer, \n",
    "#     grouped_context, \n",
    "#     source_model.config.num_hidden_layers\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=50257, bias=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.model.memory_cell.model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jit compile As: [torch.Size([1024, 768]), torch.Size([1024, 768]), torch.Size([1024, 768]), torch.Size([1024, 768]), torch.Size([1024, 768]), torch.Size([1024, 768]), torch.Size([1024, 768]), torch.Size([1024, 768]), torch.Size([1024, 768]), torch.Size([1024, 768]), torch.Size([1024, 768]), torch.Size([1024, 768])] Bs: [torch.Size([768, 64]), torch.Size([768, 64]), torch.Size([768, 64]), torch.Size([768, 64]), torch.Size([768, 64]), torch.Size([768, 64]), torch.Size([768, 64]), torch.Size([768, 64]), torch.Size([768, 64]), torch.Size([768, 64]), torch.Size([768, 64]), torch.Size([768, 64])]\n",
      "\n",
      "// Gemm operator cutlass_tensorop_bf16_s16816gemm_grouped_bf16_256x128_64x3_tt_align8\n",
      "using cutlass_tensorop_bf16_s16816gemm_grouped_bf16_256x128_64x3_tt_align8_base =\n",
      "  typename cutlass::gemm::kernel::DefaultGemmGrouped<\n",
      "    cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::ComplexTransform::kNone, 8,\n",
      "    cutlass::bfloat16_t, cutlass::layout::RowMajor, cutlass::ComplexTransform::kNone, 8,\n",
      "    cutlass::bfloat16_t, cutlass::layout::RowMajor,\n",
      "    float,\n",
      "    cutlass::arch::OpClassTensorOp,\n",
      "    cutlass::arch::Sm80,\n",
      "    cutlass::gemm::GemmShape<256, 128, 64>,\n",
      "    cutlass::gemm::GemmShape<64, 64, 64>,\n",
      "    cutlass::gemm::GemmShape<16, 8, 16>,\n",
      "    cutlass::epilogue::thread::LinearCombination<cutlass::bfloat16_t, 8, float, float>,\n",
      "    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>,\n",
      "    3,\n",
      "    cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly,\n",
      "    cutlass::arch::OpMultiplyAdd\n",
      ">::GemmKernel;\n",
      "\n",
      "// Define named type\n",
      "struct cutlass_tensorop_bf16_s16816gemm_grouped_bf16_256x128_64x3_tt_align8_type :\n",
      "  public cutlass_tensorop_bf16_s16816gemm_grouped_bf16_256x128_64x3_tt_align8_base { };\n",
      "\n",
      "USE_EFFICIENT_ALLOCATION\n"
     ]
    }
   ],
   "source": [
    "### ONLY FOR FAST LATENCY VERSION\n",
    "\n",
    "# compile full layers\n",
    "segments_input = torch.rand((source_model.config.num_hidden_layers, armt_config['segment_size'], source_model.config.hidden_size), device=\"cuda\", dtype=dtype)\n",
    "\n",
    "i, j = 0, source_model.config.num_hidden_layers\n",
    "grouped_context.start_idx = i\n",
    "grouped_context.end_idx = j\n",
    "grouped_context.is_full = True\n",
    "\n",
    "ao = associate_with_context(grouped_layer, grouped_context, segments_input[i:j])\n",
    "grouped_layer.generate_mode = True\n",
    "armt_grouped_model.memory_cell.model.transformer(inputs_embeds=segments_input[i:j], use_cache=False)\n",
    "update_mem_with_context(grouped_layer, grouped_context, segments_input[i:j])\n",
    "\n",
    "# del ao\n",
    "# del segments_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4096, 8192, 16384, 32768, 65536, 131072\n",
    "num_segments = 16384//armt_config[\"segment_size\"]\n",
    "input_ids = torch.randint(\n",
    "    0, 10000, \n",
    "    (1, num_segments*armt_config[\"segment_size\"]), \n",
    "    dtype=torch.long, \n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16384])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 846 ms, sys: 40.6 ms, total: 886 ms\n",
      "Wall time: 380 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %%timeit\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # armt_reference_model.memory_cell.zero_mem()\n",
    "    armt_reference_model.memory_cell.generate_mode(False)\n",
    "    reference_output = armt_reference_model.forward(input_ids)\n",
    "\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 152 ms, sys: 16 ms, total: 168 ms\n",
      "Wall time: 167 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %%timeit\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = executor.forward(input_ids, skip_concat=False)\n",
    "\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0325, device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(output.logits-reference_output.logits)/torch.norm(reference_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16384, 50257]), torch.Size([1, 16384, 50257]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape, reference_output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_tokens = (output.logits.argmax(1) != reference_output.logits.argmax(2)[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2242, device='cuda:0'), 16384, tensor(0.1367, device='cuda:0'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_tokens, output.logits.shape[0], missed_tokens/output.logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svtdanny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
