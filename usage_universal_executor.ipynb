{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "sys.path.append(\"./associative-recurrent-memory-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grouped_batching.llama1b_grouping import (\n",
    "    wrap_model_with_armt, get_grouped_states, \n",
    "    make_grouped_layer_from_single_layer, make_grouped_model_from_naive,\n",
    "    make_grouped_sliced_layer_from_single_layer\n",
    ")\n",
    "from grouped_batching.fast_executor import (\n",
    "    FastGroupedArmtExecutor, GroupedLayerContext, \n",
    "    associate_with_context, update_mem_with_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grouped_batching.universal_grouping import (\n",
    "    extract_params_from_module, get_universal_grouped_states,\n",
    "    get_module_by_path, make_universal_grouped_layer,\n",
    "    set_module_by_path, make_universal_grouped_model\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.bfloat16\n",
    "torch.set_default_dtype(dtype)\n",
    "torch.set_grad_enabled(False)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import traceback\n",
    "\n",
    "def add_trace_to_forward(module, msg):\n",
    "    original_forward = module.forward\n",
    "\n",
    "    def traced_forward(*args, **kwargs):\n",
    "        print(f\"\\n[TRACE] {msg} {module.__class__.__name__}.forward called from:\\n\" + ''.join(traceback.format_stack(limit=10)))\n",
    "        return original_forward(*args, **kwargs)\n",
    "\n",
    "    module.forward = traced_forward\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import copy\n",
    "\n",
    "MODEL_TYPE = \"llama\" # \"gpt2\" \"llama\"\n",
    "\n",
    "\n",
    "if MODEL_TYPE == \"gpt2\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "    source_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\"\n",
    "                                                , attn_implementation=\"flash_attention_2\"\n",
    "                                                ,torch_dtype=dtype)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"JackFram/llama-160m\")\n",
    "    source_model = AutoModelForCausalLM.from_pretrained(\"JackFram/llama-160m\"\n",
    "                                                , attn_implementation=\"flash_attention_2\"\n",
    "                                                ,torch_dtype=dtype)\n",
    "\n",
    "# Replace all LayerNorm modules in the model with LayerNorm without weights and biases\n",
    "# for name, module in source_model.named_modules():\n",
    "#     if isinstance(module, nn.LayerNorm):\n",
    "#         # Create a new LayerNorm with elementwise_affine=False (no weights and biases)\n",
    "#         new_layernorm = nn.LayerNorm(\n",
    "#             normalized_shape=module.normalized_shape,\n",
    "#             eps=module.eps,\n",
    "#             elementwise_affine=False\n",
    "#         )\n",
    "        \n",
    "#         # Get the parent module and attribute name to replace the LayerNorm\n",
    "#         parent_name = '.'.join(name.split('.')[:-1])\n",
    "#         child_name = name.split('.')[-1]\n",
    "        \n",
    "#         if parent_name:\n",
    "#             parent = source_model\n",
    "#             for part in parent_name.split('.'):\n",
    "#                 parent = getattr(parent, part)\n",
    "#             setattr(parent, child_name, new_layernorm)\n",
    "#         else:\n",
    "#             setattr(source_model, child_name, new_layernorm)\n",
    "\n",
    "# for l in source_model.transformer.h:\n",
    "    # l.mlp = nn.Identity()\n",
    "    # l.attn.attention.out_proj = nn.Identity()\n",
    "    # l.attn.attention = nn.Identity()\n",
    "\n",
    "# source_model.transformer.h = source_model.transformer.h[:1]\n",
    "\n",
    "# source_model.transformer.wpe.weight.fill_(0)\n",
    "# source_model.transformer.wpe.weight.data = torch.tensor([3,3,3], dtype=dtype)\n",
    "# source_model.transformer.wte.weight.fill_(0)\n",
    "# source_model.transformer.wte.weight.data = torch.tensor([3,3,3], dtype=dtype)\n",
    "# add_trace_to_forward(source_model.transformer.wpe, 'wpe')\n",
    "# add_trace_to_forward(source_model.transformer.wte, 'wte')\n",
    "\n",
    "\n",
    "# source_model.ln_f = source_model.transformer.ln_f\n",
    "# source_model.transformer.ln_f = nn.Identity()\n",
    "# source_model.lm_head = nn.Identity()\n",
    "reference_model = copy.deepcopy(source_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((768,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((768,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((768,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "armt_config = dict(\n",
    "    segment_size=1024,\n",
    "    num_mem_tokens=128,\n",
    "    d_mem=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((768,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((768,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((768,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if MODEL_TYPE == \"gpt2\":\n",
    "    layers_attr = 'transformer.h'\n",
    "else:\n",
    "    layers_attr = 'model.layers'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "armt_model = wrap_model_with_armt(source_model, **armt_config, layers_attr=layers_attr)\n",
    "armt_model.to(\"cuda\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "armt_reference_model = wrap_model_with_armt(reference_model, **armt_config, layers_attr=layers_attr)\n",
    "armt_reference_model.to(\"cuda\")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"gpt2\":\n",
    "    grouped_params = get_universal_grouped_states(armt_model.memory_cell.model.transformer.h)\n",
    "else:\n",
    "    grouped_params = get_universal_grouped_states(armt_model.memory_cell.model.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W_mq.weight',\n",
       " 'W_mk.weight',\n",
       " 'W_mv.weight',\n",
       " 'W_mb.weight',\n",
       " 'W_mb.bias',\n",
       " 'layer.self_attn.q_proj.weight',\n",
       " 'layer.self_attn.k_proj.weight',\n",
       " 'layer.self_attn.v_proj.weight',\n",
       " 'layer.self_attn.o_proj.weight',\n",
       " 'layer.mlp.gate_proj.weight',\n",
       " 'layer.mlp.up_proj.weight',\n",
       " 'layer.mlp.down_proj.weight',\n",
       " 'layer.input_layernorm.weight',\n",
       " 'layer.post_attention_layernorm.weight',\n",
       " 'W_mem',\n",
       " 'z']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(grouped_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_context = GroupedLayerContext()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBSTITUTE efficient module_path='W_mq': len(weight_values)=12 None \n",
      "SUBSTITUTE efficient module_path='W_mk': len(weight_values)=12 None \n",
      "SUBSTITUTE efficient module_path='W_mv': len(weight_values)=12 None \n",
      "SUBSTITUTE naive module_path='W_mb': len(weight_values)=12 12 \n",
      "SUBSTITUTE efficient module_path='layer.self_attn.q_proj': len(weight_values)=12 None \n",
      "SUBSTITUTE efficient module_path='layer.self_attn.k_proj': len(weight_values)=12 None \n",
      "SUBSTITUTE efficient module_path='layer.self_attn.v_proj': len(weight_values)=12 None \n",
      "SUBSTITUTE efficient module_path='layer.self_attn.o_proj': len(weight_values)=12 None \n",
      "SUBSTITUTE efficient module_path='layer.mlp.gate_proj': len(weight_values)=12 None \n",
      "SUBSTITUTE efficient module_path='layer.mlp.up_proj': len(weight_values)=12 None \n",
      "SUBSTITUTE efficient module_path='layer.mlp.down_proj': len(weight_values)=12 None \n",
      "SUBSTITUTE norm_path='layer.input_layernorm': torch.Size([12, 768]) None \n",
      "SUBSTITUTE norm_path='layer.post_attention_layernorm': torch.Size([12, 768]) None \n"
     ]
    }
   ],
   "source": [
    "if MODEL_TYPE == \"gpt2\":\n",
    "    layer_base = copy.deepcopy(armt_model.memory_cell.model.transformer.h[0])\n",
    "else:\n",
    "    layer_base = copy.deepcopy(armt_model.memory_cell.model.model.layers[0])\n",
    "\n",
    "grouped_layer = make_universal_grouped_layer(\n",
    "    grouped_context, \n",
    "    layer_base,\n",
    "    grouped_params,\n",
    "    use_layer_norm=(MODEL_TYPE == \"gpt2\"), # layer norm for gpt2, rms norm for llama\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x AssociativeLayerWrapper(\n",
       "        (W_mq): Linear(in_features=768, out_features=64, bias=False)\n",
       "        (W_mk): Linear(in_features=768, out_features=64, bias=False)\n",
       "        (W_mv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_mb): Linear(in_features=768, out_features=1, bias=True)\n",
       "        (layer): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (up_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (down_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((768,), eps=1e-06)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((768,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((768,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 384, 768]), torch.Size([12, 384]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_layer.W_mem.data.shape, grouped_layer.z.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grouped_batching.universal_executor import UniversalGroupedExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0): AssociativeLayerWrapper(\n",
       "        (W_mq): Linear(in_features=768, out_features=64, bias=False)\n",
       "        (W_mk): Linear(in_features=768, out_features=64, bias=False)\n",
       "        (W_mv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_mb): Linear(in_features=768, out_features=1, bias=True)\n",
       "        (layer): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (up_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (down_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((12, 1, 768), eps=1e-06)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((12, 1, 768), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((768,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"gpt2\":\n",
    "    layers_path = \"memory_cell.model.transformer.h\"\n",
    "else:\n",
    "    layers_path = \"memory_cell.model.model.layers\"\n",
    "\n",
    "armt_grouped_model, source_model_layers = make_universal_grouped_model(\n",
    "    armt_model, \n",
    "    grouped_layer,\n",
    "    layers_path=layers_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_segment_gpt2(model, input_segment):\n",
    "    pos_ids = torch.arange(\n",
    "        0, \n",
    "        input_segment.shape[0], \n",
    "        device=input_segment.device\n",
    "    )\n",
    "    wpe_emb = model.memory_cell.model.transformer.wpe(pos_ids)\n",
    "    input_segment.add_(wpe_emb)\n",
    "    return input_segment\n",
    "\n",
    "grouped_compute_gpt2 = None # uses default grouped layer\n",
    "\n",
    "def postprocess_segment_gpt2(model, output_segment):\n",
    "    out = model.memory_cell.model.transformer.ln_f(output_segment)\n",
    "    out_tokens = model.memory_cell.model.lm_head(out)\n",
    "    return out_tokens\n",
    "\n",
    "\n",
    "\n",
    "preprocess_segment_llama = None\n",
    "\n",
    "def grouped_compute_llama(model, grouped_layer, grouped_input_tensor):\n",
    "    position_ids = torch.arange(\n",
    "        0, \n",
    "        grouped_input_tensor.shape[1], \n",
    "        device=grouped_input_tensor.device\n",
    "    ).unsqueeze(0)\n",
    "    position_embeddings = model.memory_cell.model.model.rotary_emb(grouped_input_tensor, position_ids)\n",
    "    batch_output = grouped_layer.forward(grouped_input_tensor, position_embeddings=position_embeddings)\n",
    "    return batch_output\n",
    "    \n",
    "\n",
    "def postprocess_segment_llama(model, output_segment):\n",
    "    out = model.memory_cell.model.model.norm(output_segment)\n",
    "    out_tokens = model.memory_cell.model.lm_head(out)\n",
    "    return out_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"gpt2\":\n",
    "    executor = UniversalGroupedExecutor(\n",
    "        model=armt_grouped_model,\n",
    "        grouped_layer=grouped_layer,\n",
    "        context=grouped_context,\n",
    "        n_layers=source_model.config.num_hidden_layers,\n",
    "        model_path=\"memory_cell.model.transformer\",\n",
    "        out_norm_attr=\"memory_cell.model.transformer.ln_f\",\n",
    "        lm_head_path=\"memory_cell.model.lm_head\",\n",
    "        memory_path=\"memory_cell\",\n",
    "        preprocess_segment_fn = preprocess_segment_gpt2,\n",
    "        postprocess_segment_fn = postprocess_segment_gpt2,\n",
    "        grouped_compute_fn = grouped_compute_gpt2,\n",
    "    )\n",
    "else:\n",
    "    executor = UniversalGroupedExecutor(\n",
    "        model=armt_grouped_model,\n",
    "        grouped_layer=grouped_layer,\n",
    "        context=grouped_context,\n",
    "        n_layers=source_model.config.num_hidden_layers,\n",
    "        model_path=\"memory_cell.model.model\",\n",
    "        out_norm_attr=\"memory_cell.model.model.norm\",\n",
    "        lm_head_path=\"memory_cell.model.lm_head\",\n",
    "        memory_path=\"memory_cell\",\n",
    "        preprocess_segment_fn = preprocess_segment_llama,\n",
    "        postprocess_segment_fn = postprocess_segment_llama,\n",
    "        grouped_compute_fn = grouped_compute_llama,\n",
    "    )\n",
    "\n",
    "executor.vanilla_model = armt_reference_model\n",
    "\n",
    "# executor = FastGroupedArmtExecutor(\n",
    "#     armt_grouped_model, \n",
    "#     grouped_layer, \n",
    "#     grouped_context, \n",
    "#     source_model.config.num_hidden_layers\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaRMSNorm((768,), eps=1e-06)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.model.memory_cell.model.model.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ONLY FOR FAST LATENCY VERSION\n",
    "\n",
    "# compile full layers\n",
    "segments_input = torch.rand((source_model.config.num_hidden_layers, armt_config['segment_size'], source_model.config.hidden_size), device=\"cuda\", dtype=dtype)\n",
    "\n",
    "i, j = 0, source_model.config.num_hidden_layers\n",
    "grouped_context.start_idx = i\n",
    "grouped_context.end_idx = j\n",
    "grouped_context.is_full = True\n",
    "\n",
    "ao = associate_with_context(grouped_layer, grouped_context, segments_input[i:j])\n",
    "grouped_layer.generate_mode = True\n",
    "if MODEL_TYPE == \"gpt2\":\n",
    "    armt_grouped_model.memory_cell.model.transformer(inputs_embeds=segments_input[i:j], use_cache=False)\n",
    "else:\n",
    "    armt_grouped_model.memory_cell.model.model(inputs_embeds=segments_input[i:j], use_cache=False)\n",
    "update_mem_with_context(grouped_layer, grouped_context, segments_input[i:j])\n",
    "\n",
    "# del ao\n",
    "# del segments_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4096, 8192, 16384, 32768, 65536, 131072\n",
    "num_segments = 4096//armt_config[\"segment_size\"]\n",
    "input_ids = torch.randint(\n",
    "    0, 10000, \n",
    "    (1, num_segments*armt_config[\"segment_size\"]), \n",
    "    dtype=torch.long, \n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 514 ms, sys: 21.9 ms, total: 536 ms\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %%timeit\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # armt_reference_model.memory_cell.zero_mem()\n",
    "    armt_reference_model.memory_cell.generate_mode(False)\n",
    "    reference_output = armt_reference_model.forward(input_ids)\n",
    "\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.1 ms, sys: 0 ns, total: 64.1 ms\n",
      "Wall time: 63.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %%timeit\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = executor.forward(input_ids, skip_concat=False)\n",
    "\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(output.logits-reference_output.logits)/torch.norm(reference_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4096, 32000]), torch.Size([1, 4096, 32000]))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape, reference_output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_tokens = (output.logits.argmax(1) != reference_output.logits.argmax(2)[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(576, device='cuda:0'), 4096, tensor(0.1406, device='cuda:0'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_tokens, output.logits.shape[0], missed_tokens/output.logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "    'max_new_tokens': 20,\n",
    "    'pad_token_id': 0,\n",
    "    'eos_token_id': 1,\n",
    "    'attention_mask': None,\n",
    "    # 'attention_mask': torch.tril(torch.ones((1024, 1024), device='cuda', dtype=bool))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_out_ref = armt_reference_model.generate(input_ids, **generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[29908,  1159,   376, 29908, 29908, 29908, 29908, 29908, 29908, 29908,\n",
       "         29908, 29908, 29908, 29908, 29908, 29908, 29908, 29908, 29908, 29908]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_out_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3072]) torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "gen_out = executor.generate(input_ids, seg_size=1024+128, **generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[29908,  1159,   376, 29908, 29908, 29908, 29908, 29908, 29908, 29908,\n",
       "          29908, 29908, 29908, 29908, 29908, 29908, 29908, 29908, 29908, 29908]],\n",
       "        device='cuda:0'),\n",
       " 0.0005311965942382812)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svtdanny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
